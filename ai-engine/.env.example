# Apollo AI Engine - Environment Configuration
#
# SpikingBrain-7B Configuration (Primary - Local Inference)
# =========================================================

# vLLM Server URL - The endpoint where SpikingBrain-7B is served
# Default: http://localhost:8000 (local vLLM server)
SPIKINGBRAIN_API_URL=http://localhost:8000

# API Key (optional for local deployments, required for remote)
SPIKINGBRAIN_API_KEY=

# Model variant: 'base', 'sft' (recommended), or 'quantized'
# base: Panyuqi/V1-7B-base - Pre-trained base model
# sft: Panyuqi/V1-7B-sft-s3-reasoning - Fine-tuned with reasoning (recommended)
# quantized: Abel2076/SpikingBrain-7B-W8ASpike - Quantized for efficiency
SPIKINGBRAIN_MODEL_VARIANT=sft

# Tensor parallel size (for multi-GPU inference)
SPIKINGBRAIN_TENSOR_PARALLEL=1

# Maximum tokens to generate
SPIKINGBRAIN_MAX_TOKENS=4096

# Temperature for sampling (0.0-1.0)
SPIKINGBRAIN_TEMPERATURE=0.7

# Top-p (nucleus) sampling
SPIKINGBRAIN_TOP_P=0.9

# Request timeout in milliseconds
SPIKINGBRAIN_TIMEOUT=60000

# Maximum retries on failure
SPIKINGBRAIN_MAX_RETRIES=3

# Retry delay in milliseconds (uses exponential backoff)
SPIKINGBRAIN_RETRY_DELAY=1000

# Enable streaming responses (true/false)
SPIKINGBRAIN_STREAMING=true


# Model Router Configuration
# ==========================

# Default AI model to use (spikingbrain-7b, claude-3-sonnet, gpt-4, etc.)
DEFAULT_AI_MODEL=spikingbrain-7b

# Prefer local models over cloud when available (true/false)
USE_LOCAL_MODEL_FIRST=true

# Enable local inference with SpikingBrain (true/false)
USE_LOCAL_INFERENCE=true


# Cloud Model API Keys (Fallback)
# ===============================

# Anthropic Claude
ANTHROPIC_API_KEY=

# OpenAI GPT-4
OPENAI_API_KEY=

# Google Gemini
GOOGLE_API_KEY=

# DeepSeek (optional)
DEEPSEEK_API_URL=https://api.deepseek.com/v1
DEEPSEEK_API_KEY=


# BugTrace-AI Configuration
# =========================

# Analysis depth (number of personas, recommended: 3-5)
BUGTRACE_ANALYSIS_DEPTH=5

# Enable AI-powered consolidation of findings
BUGTRACE_ENABLE_CONSOLIDATION=true

# Enable deep analysis refinement
BUGTRACE_ENABLE_DEEP_ANALYSIS=false

# Maximum concurrent persona analyses
BUGTRACE_MAX_CONCURRENT=3

# Analysis timeout in milliseconds
BUGTRACE_ANALYSIS_TIMEOUT=120000


# Cyberspike Villager Configuration
# =================================

# Enable autonomous operations (requires authorization)
VILLAGER_AUTONOMOUS_MODE=false

# Operation logging level (debug, info, warn, error)
VILLAGER_LOG_LEVEL=info

# Maximum operation runtime in seconds
VILLAGER_MAX_OPERATION_TIME=3600


# vLLM Server Setup (for local deployment)
# ========================================
#
# To run SpikingBrain-7B locally with vLLM:
#
# 1. Install vLLM with hymeta plugin:
#    pip install vllm==0.10.0 vllm-hymeta
#
# 2. Download model weights:
#    huggingface-cli download Panyuqi/V1-7B-sft-s3-reasoning
#
# 3. Remove auto_map from config.json (required for vLLM)
#
# 4. Start vLLM server:
#    vllm serve Panyuqi/V1-7B-sft-s3-reasoning \
#      --served-model-name spikingbrain-7b \
#      --dtype bfloat16 \
#      --tensor-parallel-size 1 \
#      --host 0.0.0.0 \
#      --port 8000
#
# For multi-GPU:
#    vllm serve Panyuqi/V1-7B-sft-s3-reasoning \
#      --served-model-name spikingbrain-7b \
#      --dtype bfloat16 \
#      --tensor-parallel-size 2 \
#      --pipeline-parallel-size 1
